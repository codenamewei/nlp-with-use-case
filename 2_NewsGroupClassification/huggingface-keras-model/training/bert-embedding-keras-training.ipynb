{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 News Group Data Classification\n",
    "\n",
    "Embeddings generation using Hugging Face Bert Pretrained Model and News Group classifier trained with multi-backend Keras.  \n",
    "The model generated is then served with konduit-serving for REST inference.\n",
    "\n",
    "Konduit-Serving: https://github.com/KonduitAI/konduit-serving  \n",
    "Hugging Face NLP Library: https://github.com/huggingface/transformers  \n",
    "Data: http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-05c945bcd6fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mppb\u001b[0m \u001b[1;31m#!python -m pip install transformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers as ppb #!python -m pip install transformers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = 'D:\\\\Users\\\\chiawei\\\\konduit\\\\Github\\\\newsgroup_data\\\\20news-bydate\\\\'\n",
    "train_folder = '20news-bydate-train'\n",
    "test_folder = '20news-bydate-test'\n",
    "file_path = 'D:\\\\Users\\\\chiawei\\\\konduit\\\\Github\\\\rpa-email-forwarder\\\\files\\\\'\n",
    "MAX_TOKENIZE_LEN = 512\n",
    "\n",
    "class_label = [f for f in os.listdir(os.path.join(data_root_path, train_folder))]\n",
    "class_index = [i for i in range(len(class_label))]\n",
    "\n",
    "total_class = len(class_index)\n",
    "\n",
    "label_index_pair = {}\n",
    "for label, index in zip(class_label, class_index):\n",
    "        label_index_pair[label] = index\n",
    "        print(label, index)\n",
    "        \n",
    "index_label_pair = {}\n",
    "for index, label in zip(class_index, class_label):\n",
    "        index_label_pair[index] = label\n",
    "\n",
    "print('Save index label')\n",
    "label_path = \"labelclass.pickle\"\n",
    "with open(label_path, 'wb') as labelhandler:\n",
    "    pickle.dump(index_label_pair, labelhandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_regex(text):\n",
    "    \n",
    "    # Applies preprocessing on text\n",
    "    \n",
    "    #remove leading & end white spaces and convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove punctuation marks \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for i in text:\n",
    "        if i in punctuations: \n",
    "                text = text.replace(i, \"\")\n",
    "            \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    \n",
    "    #remove number\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"from\", \"to\", \"subject\", \"title\", \"request\", \"looking\", \"look\", \"forward\", \"cheers\", \"regards\", \"thank\", \"thanks\", \"hi\", \"all\", \"since\", \"mentioned\", \"free\", \"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"]\n",
    "\n",
    "def remove_stop_words(input_str):\n",
    "    \n",
    "    tokenized_words = input_str.split()\n",
    "    \n",
    "    filtered_words = [w for w in tokenized_words if not w in stop_words]\n",
    "    \n",
    "    output = \" \".join(filtered_words)\n",
    "    \n",
    "    if len(output) > MAX_TOKENIZE_LEN:\n",
    "        return output[0: MAX_TOKENIZE_LEN]\n",
    "    \n",
    "    return output  #return as string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfs(data_path, class_dict):\n",
    "    \n",
    "    data = pd.DataFrame(columns = [\"text\", \"classindex\", \"classlabel\"])\n",
    "\n",
    "    text = []\n",
    "    class_index = []\n",
    "    class_label = []\n",
    "    \n",
    "    for label in label_index_pair.keys():\n",
    "\n",
    "        class_path = os.path.join(data_path, label)\n",
    "        files_list = [f for f in os.listdir(class_path) ]\n",
    "\n",
    "        for f in os.listdir(class_path):\n",
    "\n",
    "            with open(os.path.join(class_path, f), \"r\") as reader:\n",
    "\n",
    "                text.append(remove_stop_words(preprocess_regex(reader.read())))\n",
    "                class_label.append(label)\n",
    "                class_index.append(class_dict[label])\n",
    "                \n",
    "    data[\"text\"] = text\n",
    "    data[\"classindex\"] = class_index\n",
    "    data[\"classlabel\"] = class_label\n",
    "                \n",
    "    return data\n",
    "\n",
    "                    \n",
    "train_data = get_dfs(os.path.join(data_root_path, train_folder), label_index_pair)\n",
    "test_data = get_dfs(os.path.join(data_root_path, test_folder), label_index_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle data\n",
    "train_data = train_data.reindex(np.random.permutation(train_data.index))\n",
    "test_data = test_data.reindex(np.random.permutation(test_data.index))\n",
    "\n",
    "print(\"Number of training data: {}\".format(train_data.shape[0]))\n",
    "print(\"Number of testing data: {}\".format(test_data.shape[0]))\n",
    "\n",
    "train_data.head(20)['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting subset of data due to memory overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[0: 6000] #6000\n",
    "test_data = test_data[0: 2000] #2000\n",
    "#train_data.to_csv(os.path.join(data_root_path, \"train_data.csv\"))\n",
    "#test_data.to_csv(os.path.join(data_root_path, \"test_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Generation:  \n",
    "Loading hugging face transformer bert pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## uncomment below for  BERT instead of distilBERT\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization\n",
    "Tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = train_data['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_test_data = test_data['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized_train_data.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        \n",
    "padded_train_data = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train_data.values])\n",
    "padded_test_data = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test_data.values])\n",
    "\n",
    "#print(\"Shape of input data: {}\".format(padded_train_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking of padded data  \n",
    "Masking tells the NLP model to ignore (mask) the padding added when it's processing its input.  \n",
    "That's what attention_mask is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attention_mask = np.where(padded_train_data != 0, 1, 0)\n",
    "test_attention_mask = np.where(padded_test_data != 0, 1, 0)\n",
    "train_attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get embeddings through Hugging Face Bert using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 512\n",
    "epoch_count = 20\n",
    "labels = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrain_input_ids = torch.tensor(padded_train_data)  \n",
    "pytrain_attention_mask = torch.tensor(train_attention_mask)\n",
    "\n",
    "pytrain_input_ids = torch.tensor(pytrain_input_ids).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(pytrain_input_ids, attention_mask=pytrain_attention_mask)\n",
    "    \n",
    "train_features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "train_labels = np.expand_dims(train_data[\"classindex\"], axis = 1)\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pytest_input_ids = torch.tensor(padded_test_data)  \n",
    "pytest_attention_mask = torch.tensor(test_attention_mask)\n",
    "\n",
    "pytest_input_ids = torch.tensor(pytest_input_ids).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(pytest_input_ids, attention_mask=pytest_attention_mask)\n",
    "    \n",
    "test_features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "test_labels = np.expand_dims(test_data[\"classindex\"], axis = 1)\n",
    "\n",
    "test_labels = keras.utils.to_categorical(test_labels, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings with tf-backend Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(250, activation='relu', input_shape=(768,)))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(250, activation='relu'))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(labels, activation='softmax'))\n",
    "\n",
    "classifier.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "classifier.fit(train_features, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          validation_data = (test_features, test_labels),\n",
    "          epochs=50)#epoch_count)\n",
    "\n",
    "classifier.save('bert-embeddings-keras-mlp.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
